---
title: "Prediction of Heart Diseases"
author: "Raghav"
date: "14/10/2019"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

## Dataset Information:-

This data set dates from 1988 and consists of four databases: Cleveland, Hungary, Switzerland, and Long Beach V. The "target" field refers to the presence of heart disease in the patient. It is integer valued 0 = no disease and 1 = disease.

### Objective:-

The main objective is to predict whether the given person is having *heart disease* or not, with the help of several factor which are causing eg- age,cholesterol level,type of chest pain etc.

The algorithm which we are using in this problem are:-


  * Binary Logistic Regression.

  * Naive Bayes algorithm

  * Decision Tree

  * Random Forest


### Description of the Dataset:-

The data has 303 observations and 14 variables. Each observation contains following infomation about an individual.

 * **age:-** Age of individual in years
 
 * **sex:-** Gender Of individual(1 = male; 0 = female)
 
 * **cp** - Chest pain type (1 = typical angina; 2 = atypical angina; 3 = non-anginal pain; 4 = asymptomatic)
 
 *	**trestbps** - Resting blood pressure (in mm Hg on admission to the hospital)
 
 *	**chol** - Serum cholesterol in mg/dl
 
 *	**fbs** - Fasting blood sugar level > 120 mg/dl (1 = true; 0 = false)
 
 *	**restecg** - Resting electrocardiographic results (0 = normal; 1 = having ST-T; 2 = hypertrophy)
 
 *	**thalach** - Maximum heart rate achieved
 
 *	**exang** - Exercise induced angina (1 = yes; 0 = no)
 
 *	**oldpeak** - ST depression induced by exercise relative to rest
 
 *	**slope** - The slope of the peak exercise ST segment (1 = upsloping; 2 = flat; 3 = downsloping)
 
 *	**ca** - Number of major vessels (0-4) colored by flourosopy
 
 *	**thal** -Thalassemia is an inherited blood disorder that affects the body's ability to produce hemoglobin and red blood cells. 1 = normal; 2 = fixed defect; 3 = reversable defect
 
 *	**target** - the predicted attribute - diagnosis of heart disease (angiographic disease status) (Value 0 = < 50% diameter narrowing; Value 1 = > 50% diameter narrowing)

Loading the data in Rstudio
```{r}
heart<-read.csv("Z:/Data Science data/project/heart.csv",header = T)
```
header = T means that the given data has its own heading or otherwords the first observation is also considered for prediction.

```{r}
head(heart)
# we use head function when we want to see and check the first six observation of our data.
```

```{r}
tail(heart)
# same as head but it show show bottom six obervation of our data.
```
```{r}
colSums(is.na(heart))
# This function is used to check whether our data contains any NA valus or not.
# As there are no NA found we can move forward or else we have to remove NA before moving forward. 
```
To check the structure of our data
```{r}
str(heart)
```

To see the summary of our data
```{r}
summary(heart)
```

**As observing the above summary and str we can say following points:-**
 
  * **sex** cannot be continuous variable as it can be either Male or Female as per our description. Hence we have to convert the variable name *sex* from integer to factor. And also labelling it to avoid any further confusion.
 
  * **cp** cannot be continuous variable as it is type of chest pain. As it is type of chest pain, we have to convert variable *cp* to factor and labelling it to our convenience.
 
  * **fbs** cannot be continuous variable or integer as it shows blood sugar level below 120mg/dl or not.Therefore, we convert it to factor and labelling it to our convenience.
 
  * **restecg** should be factor as it is type of ECG results.Hence, it can't be integer.So, we are converting it to factor and labelling.
 
  * **exang** should be factor as per the description of the dataset. Angina can happen or not i.e. it can be either yes or no. Therefore, converting the variable to factor and labelling it.
 
  * **slope** cannot be integer as it is type of slope which is observed in ECG.Therefore we are converting the variable to factor and labelling it.
 
  * **ca** as per the description of our dataset. It can't be integer. Therefore, we are converting the variable to factor.
 
  * **thal** cannot be integer as it is type of thalassemia which cannot be numeric or integer.Therefore, we are converting the variable to factor and labelling it.
 
  * **target** is the predicated variable and tells us whether the individual has heart disease or not. Therefore, we are converting the variable to factor and labelling it for your convenience.
  
  
**According to above observation we implementing the changes**
```{r}
heart$sex<-as.factor(heart$sex)
levels(heart$sex)<-c("Female","Male")
heart$cp<-as.factor(heart$cp)
levels(heart$cp)<-c("typical","atypical","non-anginal","asymptomatic")
heart$fbs<-as.factor(heart$fbs)
levels(heart$fbs)<-c("False","True")
heart$restecg<-as.factor(heart$restecg)
levels(heart$restecg)<-c("normal","stt","hypertrophy")
heart$exang<-as.factor(heart$exang)
levels(heart$exang)<-c("No","Yes")
heart$slope<-as.factor(heart$slope)
levels(heart$slope)<-c("upsloping","flat","downsloping")
heart$ca<-as.factor(heart$ca)
heart$thal<-as.factor(heart$thal)
levels(heart$thal)<-c("normal","fixed","reversable")
heart$target<-as.factor(heart$target)
levels(heart$target)<-c("No", "Yes")
```

Checking whether the above changes are implemented or not
```{r}
str(heart)
```
```{r}
summary(heart)
```


## EDA

**EDA** stands for Exploratory Data Analysis which is an approach/philosophy for data analysis that employs a variety of techniques (mostly graphical) to maximize insight into a dataset.

For Graphical representation we require library *"ggplot2"*
```{r}
library(ggplot2)
ggplot(heart,aes(x=age,fill=target,color=target)) + geom_histogram(binwidth = 1,color="black") + labs(x = "Age",y = "Frequency", title = "Heart Disease w.r.t. Age")
```

We can conclude that the age group of 40 to 60 has the highest probability of getting heart diseases compared to age above 60.




```{r}
mytable <- table(heart$cp)
pct<-round(mytable/sum(mytable)*100)
lbls1<-paste(names(mytable),pct)
lbls<-paste(lbls1, "%", sep="")
pie(mytable, labels = lbls,col = rainbow(length(lbls)),main="Pie Chart of Chest Pain",radius = 0.9)
```

we can conclude that out of all types of chest pain, most observed in the individual are typical type of chest pain, then comes the non-anginal.


## Performing Machine learning algorithm
### Logistic Regression:-

Firstly, we are dividing our dataset into training(75%) and testing data(25%).

```{r}
set.seed(100) 
#100 is used to control the sampling permutation to 100. 
index<-sample(nrow(heart),0.75*nrow(heart))
train<-heart[index,]
test<-heart[-index,]
```


Model generation on training data and then validating the model with testing data.
```{r}
modelblr<-glm(target~.,data = train,family = "binomial")
# family = " binomial" means it contains only two outcomes.
```


To check how well our model is generated,we need to calculate predicted score and built confusion matrix to know the accuracy of the model.
```{r}
train$pred<-fitted(modelblr)
# fitted can be used only to get predicted score of the data on which model has been generated.
head(train)
```
As we can see that the predicted score are in probability of having heart diseases.But we have to find a proper cutoff points from which it is easy to distinguish between having heart diseases and not having it.

For that we require ROC curve(receiver operating characteristic curve) which is a graph showing the performance of a classification model at all classification thresholds. It will allow us to take proper cutoff.

```{r}
library(ROCR)
pred<-prediction(train$pred,train$target)
perf<-performance(pred,"tpr","fpr")
plot(perf,colorize = T,print.cutoffs.at = seq(0.1,by = 0.1))
```

With the use of ROC curve we can observe that 0.6 is having better sensitivity and specificity.There we select 0.6 as our cutoff to distinguish.


```{r}
train$pred1<-ifelse(train$pred<0.6,"No","Yes")
library(caret)
confusionMatrix(factor(train$pred1),train$target)
```
```{r}
# Accuracy of training data
acc_tr<-(109+92)/(227);acc_tr 
```
From confusion matrix of training data, we come to know that our model is 88.55% accurate.



Now validating the model on testing data
```{r}
test$pred<-predict(modelblr,test,type = "response")
# type = "response" is used to get the outcome in the form of probability of having heart diseases.
head(test)
```
As we know that, for training data the cutoff has been 0.6.Similarily the testing data will also have the same thersold or cutoff.

```{r}
test$pred1<-ifelse(test$pred<0.6,"No","Yes")
confusionMatrix(factor(test$pred1),test$target)
```
```{r}
# Accuracy of Testing data.
acc_tt<-(25+37)/(76);acc_tt
```

**To check how much of our predicted values lie inside the curve**
```{r}
auc<-performance(pred,"auc")
auc@y.values
```
We can conclude that we are getting an accuracy of 81.58% with 90.26% of our predicted values lying under the curve. Also our misclassifcation rate is 18.42%


### Naive Bayes algorithm

We need to remove the extra coloumns we added while performing BLR before implementing Naive Bayes algorithm.
```{r}
train$pred<-NULL
train$pred1<-NULL
test$pred<-NULL
test$pred1<-NULL
```

```{r}
# library(e1071) contains the naivebayes model, for that reason we have to first call this library.
library(e1071)
model_nb<-naiveBayes(target~.,data = train)
```

Checking the model with training data and creationg its confusion matrix to know how accurate the model is.
```{r}
train$pred<-predict(model_nb,train)
confusionMatrix(train$pred,train$target)
```
```{r}
acc_tr_nb<-(85+109)/227;acc_tr_nb
```
We can say that naive bayes algorithm is 85.46% accurate with the training data.

Now, Validating the model with the testing data by predicting and creating confusion matrix.
```{r}
test$pred<-predict(model_nb,test)
confusionMatrix(test$pred,test$target)
```
```{r}
acc_tt_nb<-(23+37)/76;acc_tt_nb
```
we can conclude that the model generated with help of Naive Bayes algorithm is 78.95% accurate  or we can also say that the misclassification rate for Naive Bayes algorithm is 21.05%. 


### Decision Tree

We need to remove the extra coloumns we added while performing Naive Bayes algorithm before implementing Decision Tree.
```{r}
train$pred<-NULL
test$pred<-NULL
```

We need the following libraries to perform Decision tree

  * library(rpart)
  * library(rpart.plot)


**rpart** stands for Recursive partitioning and regression trees.

rpart is used when both independent and dependent variables are continuous or categorical.

rpart automatically detects whether to perform regression or classification based on dependent variable. There is no need to specify.

**Implementing Decision tree**
```{r}
library(rpart)
tree<-rpart(target~.,method = "class",data = train)
library(rpart.plot)
rpart.plot(tree)
```

With the help of decision tree we can say that most significant variable out of all are **cp**,**ca**,**thal**,**oldpeak**.

Let's validate the model with testing data and find out the accuracy of model.
```{r}
test$pred<-predict(tree,test,type = "class")
confusionMatrix(test$pred,test$target)
```
```{r}
acc_tr_tree<-(21+37)/76;acc_tr_tree
```
we can say that decision tree is 76.32% accurate or it's misclassification rate is 23.68%.


### Random Forest

We need to remove the extra coloumns we added while performing Decision Tree before implementing Random Forest.
```{r}
test$pred<-NULL
```

In random forest, we don't require to split the data into training and testing data.We direct generate the model on the whole data.
To generate we require the library **random forest**
```{r}
library(randomForest)
# Set.seed controls the randomness by limitimg the permutation.
set.seed(100)
model_rf<-randomForest(target~.,data = heart)
model_rf
```
To plot the random forest on graph with respect to class error.
```{r}
plot(model_rf)
```

Red line represents MCR of class not having heart diseases, green line represents MCR of class having heart diseases and black line represents overall MCR or OOB error. Overall error rate is what we are interested in which seems considerably good.





## Conclusion

After performing various classification techniques and taking into account their accuracies, we can conclude all the models had an accuracy ranging from 76% to 84%. Out of which Random forest gave a slightly better accuracy of 83.5%